# Snowflake â€” Constraints & Data Validation  
## Full Teacher-Style Q&A (Detailed)

... (content above remains the same until scenario section) ...

## ğŸ”Ÿ SCENARIOâ€‘BASED QUESTIONS

### â“ Customer emails should never be NULL â€” how do we ensure this?
In Snowflake, NOT NULL will not physically block NULL values.  
So the correct mindset is:

**Design rule lives in metadata â€” enforcement lives in pipelines.**

Best practice approach:
1. Define the column as NOT NULL (for documentation + BI tools).  
2. Add validation in ETL:
   - reject records with missing email
   - or route them to a quarantine/error table
3. Include monitoring so violations are detected early.

Lesson:
> Constraints describe expectations. Pipelines enforce behavior.

---

### â“ Orders table must always link to a customer â€” what constraint concept applies?
This is a **FOREIGN KEY relationship**.

Conceptually:
orders.customer_id â†’ customers.customer_id

But Snowflake wonâ€™t stop an â€œorphan orderâ€ from being inserted.

So ETL should:
- validate customer exists before inserting
- log failures
- optionally block load

---

### â“ Numbers are rounding incorrectly â€” which type should we use?
Use **NUMBER/DECIMAL**, not FLOAT.

FLOAT is approximate and introduces rounding noise, which becomes very visible in financial reporting.

Teach your team:
> FLOAT = science calculations  
> NUMBER = business and money

---

### â“ Financial calculations show precision issues â€” FLOAT or NUMBER?
Always NUMBER.

Finance requires auditability and exact precision.

---

### â“ JSON logs contain dynamic fields â€” how should we store them?
Store initially in **VARIANT**.

Then:
- identify frequently-used attributes
- model those fields into structured columns later

This is the healthy lifecycle:
1. land JSON fast
2. observe real usage
3. normalize what matters

---

### â“ ETL inserts empty strings instead of NULL â€” what is the impact?
Dashboards show blanks instead of â€œmissingâ€ values.

This leads to:
- incorrect counts
- misinterpreted completeness
- silent data quality problems

Fix in ETL:
- standardize blanks â†’ NULL where appropriate

---

### â“ Need to analyze delivery routes â€” which data type should we use?
Use **GEOGRAPHY**.

Benefits:
- validates shapes
- supports distance calculations
- allows advanced spatial queries

Avoid storing coordinates as plain text when spatial analysis is required.

---

### â“ Team wants constraints enforced â€” where should they enforce instead?
Enforce using:
- ETL pipelines
- dbt tests
- DQ frameworks (Great Expectations, Soda, etc.)

Snowflake stores rules â€” pipelines enforce rules.

---

### â“ Analysts need to understand relationships â€” how do constraints help?
Even though not enforced, constraints:

- reveal PK/FK relationships visually
- improve BI auto-joins
- reduce onboarding time
- communicate business meaning

They are part of good data modeling discipline.

---

## 1ï¸âƒ£1ï¸âƒ£ TRICK / MISCONCEPTIONS

### âŒ â€œPRIMARY KEY enforces uniqueness in Snowflake.â€
Wrong â€” it does NOT enforce anything.

PRIMARY KEY is **informational only**.

---

### âŒ â€œSnowflake rejects invalid foreign key relationships.â€
Wrong â€” orphan rows are allowed.

That is why pipeline validation exists.

---

### âŒ â€œNOT NULL guarantees clean data.â€
No â€” NULLs can still appear if upstream systems insert them.

---

### âŒ â€œVARIANT means schema-less forever.â€
No â€” schema simply moves from **write time** to **read time**.

Eventually, highâ€‘value JSON fields should be modeled.

---

### âŒ â€œUsing JSON means we donâ€™t need data modeling anymore.â€
Completely false.

Modeling becomes **more important**, not less â€” otherwise analytics becomes unmanageable.

---

### âŒ â€œConstraints are useless in a data warehouse.â€
Incorrect.

They provide:
- documentation
- clarity
- BI intelligence
- lineage support
- shared understanding

Snowflake intentionally separates **description** from **enforcement** â€” but both still matter.

---

## ğŸ¯ Key Takeaway
> Constraints tell the truth about how data *should* behave.  
> Pipelines ensure data actually behaves that way.
