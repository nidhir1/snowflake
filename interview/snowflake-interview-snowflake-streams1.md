â„ï¸ Snowflake Streams â€” Deep, Real-World Explanation (CDC + Auditing)
ğŸ”¹ 1ï¸âƒ£ WORKING WITH SNOWFLAKE STREAMS
A. BASICS â€” WHAT IS A STREAM?
â“ What is a Snowflake Stream?

A Stream is a change-tracking object attached to a table (or view) that records what changed since the last time someone read it.

It tracks:

Type of change	Example
Insert	New order added
Update	Customer email corrected
Delete	Old record removed

Important:

Streams do not store copies of rows.
They store references to changes recorded by Time Travel.

You should imagine it like:

ğŸ“Œ â€œSticky notes attached to the table telling you what was changed.â€

â“ Why do we need Streams?

Without streams, every incremental pipeline would require:

âŒ Full table reloads
âŒ Expensive join comparisons
âŒ Complex â€œdelta detectionâ€ logic
âŒ Increased compute cost
âŒ Higher latency

Streams let you ask:

â€œGive me ONLY the rows that changed since last run.â€

This makes pipelines:

âœ” efficient
âœ” predictable
âœ” cheaper
âœ” reliable

â“ What problem do Streams solve in ETL pipelines?

Typical ETL question:

â€œYesterday I loaded data â€” what changed since then?â€

Streams answer that automatically.

They are perfect for:

near real-time ingestion pipelines

replication to downstream systems

auditing and reconciliation

CDC workloads (Change Data Capture)

â“ Are Streams physical data copies?

No â€” and this is critical.

A Stream does not:

âœ– duplicate data
âœ– copy tables
âœ– store entire records

Instead, it keeps pointers to modified micro-partitions.

This means:

âœ” low storage usage
âœ” always consistent with source
âœ” minimal maintenance

â“ How do Streams track table changes?

When data changes, Snowflake stores new versions using:

âœ… Time Travel
âœ… metadata versioning

A Stream reads those versions and exposes only differences.

Think of it as:

ğŸ“· snapshot (before)
ğŸ“· snapshot (after)
ğŸ“ stream shows differences

â“ What is CDC?

CDC = Change Data Capture

Goal:

Detect data changes and deliver them downstream safely.

Snowflake Streams = built-in CDC engine.

â“ What operations can Streams capture?

Depends on stream type:

Stream Type	Inserts	Updates	Deletes
Standard	âœ”	âœ”	âœ”
Append-Only	âœ”	âœ–	âœ–
Insert-Only	âœ”	âœ–	âœ–
â“ Do Streams modify underlying tables?

Never.

Streams are passive observers.
They record â€” they donâ€™t change.

â“ How often can a Stream be queried?

Unlimited â€” but:

Every read advances consumption state.

Meaning: old changes will not appear again.

â“ What privileges are needed?

User must have:

USAGE on database + schema

OWNERSHIP or SELECT on table

CREATE STREAM privilege on schema

B. STREAM OBJECT & DML AUDITING
â“ What metadata does a Stream store?

It stores:

what changed

table version at change time

consumption pointer (bookmark)

It does NOT store:

âœ– permanent history
âœ– full copies
âœ– user info

â“ What are METADATA$ columns?

Streams automatically return extra fields.

Column	Meaning
METADATA$ACTION	INSERT or DELETE
METADATA$ISUPDATE	True if part of an Update
METADATA$ROW_ID	Internal identifier for row linkage
METADATA$VERSION	Table version
METADATA$DELETE	Marks logical delete

These allow you to reconstruct history.

â“ METADATA$ACTION

Updates are NOT stored as one operation.

Update transforms into:

1ï¸âƒ£ delete old version
2ï¸âƒ£ insert new version

So you can see change over time.

â“ METADATA$ISUPDATE

True when action belongs to an UPDATE event.

Allows differentiation between:

actual delete

â€œdelete caused by updateâ€

â“ METADATA$ROW_ID

Unique row fingerprint.

Crucial for matching update before/after values.

â“ Can Streams detect TRUNCATE?

No â€” TRUNCATE bypasses CDC.

Production pattern:

ğŸ‘‰ Avoid TRUNCATE on tables with streams
ğŸ‘‰ Use DELETE with WHERE when auditing needed

â“ Do Streams track changes forever?

No.

Streams rely on Time Travel.

Once retention expires:

âŒ Stream becomes STALE
âŒ Changes cannot be recovered

â“ What happens after a stream is consumed?

bookmark moves

Snowflake remembers latest processed point

next query returns only new changes

Consumption does NOT delete data â€” it only advances marker.

C. STREAM TYPES (DETAILED)
â­ Standard Stream

Captures:

âœ” Full CDC
âœ” Before/after states

Use when:

syncing to downstream systems

slowly changing dimensions

operational CDC replication

back-auditing business transactions

â­ Append-Only Stream

Tracks inserts only.

Perfect for:

log ingestion

append-only datasets

telemetry

clickstream

event pipelines

Lighter â†’ lower compute.

â­ Insert-Only Stream

Even lighter.

Used when:

every row is brand new

no updates/deletes ever occur

downstream only appends

â“ Why choose lighter stream types?

Because unnecessary metadata costs:

compute

memory

storage

Rule:

Choose smallest stream that meets business need.

D. DATA FLOW WITH STREAMS
â“ What is source table?

Table receiving actual data.

â“ What is target table?

Transformed table where cleaned/processed records go.

â“ Streams + Tasks = Pipeline

Typical pattern:

1ï¸âƒ£ Data lands in table
2ï¸âƒ£ Stream records changes
3ï¸âƒ£ Task runs every X minutes
4ï¸âƒ£ Reads stream
5ï¸âƒ£ Applies logic to target table

This gives:

âœ” near real-time ETL
âœ” consistent incremental processing

â“ What happens when ETL reads stream?

changes delivered

only new rows appear

stream position moves forward

If ETL crashes before commit:

ğŸ‘‰ pointer does not move
ğŸ‘‰ safe retry

â“ Offset checkpoint

Internal pointer telling Snowflake:

â€œUp to here, the pipeline has already processed changes.â€

â“ Can one stream feed multiple consumers?

No.

Each consumer requires its own stream.

Otherwise:

âŒ one consumer steals changes from others.

â“ Can one table have multiple streams?

Yes â€” and common.

Example:

stream for ETL

stream for ML feature refresh

stream for audit

â“ Reset / recreate stream?

You must drop + recreate.

You may lose older data depending on retention.

ğŸ”¹ 2ï¸âƒ£ TIME TRAVEL WITH STREAMS
â“ How do Streams rely on Time Travel?

Streams read history stored by Time Travel.

If history disappearsâ€¦

Stream breaks.

â“ What is stale stream error?

Means:

Stream needs history that no longer exists.

To fix:

recreate stream

restore clone older than retention

extend retention next time

â“ What if table is dropped?

If recovered through:

âœ” UNDROP
âœ” CLONE

Then stream can sometimes be restored â€” depending on snapshot.

â“ Do Streams survive Fail-Safe?

Only indirectly.

Fail-Safe protects data, not streams.

If base table is restored, stream may still need reconstruction.

Best Practices

âœ” consume streams frequently
âœ” avoid long idle periods
âœ” avoid truncate on CDC tables
âœ” increase retention if pipelines are slow
âœ” donâ€™t treat streams as permanent audit systems

ğŸ”¹ 3ï¸âƒ£ AUDITING INSERT / UPDATE / DELETE (IN DEPTH)
Inserts

Rows appear flagged INSERT.

Updates

Appear as:

DELETE (old row)
INSERT (new row)

Deletes

Appear as DELETE only.

â“ Do Streams track WHO changed data?

No.

To answer WHO, use:

QUERY_HISTORY

ACCESS_HISTORY

Join them together.

â“ What if a stream is read twice accidentally?

You cannot rewind.

Options:

restore clone and replay

rebuild ETL checkpoint logic

ğŸ”¹ 4ï¸âƒ£ SCENARIOS â€” HOW A REAL ENGINEER THINKS

Instead of short answers, we explain:

ğŸ‘‰ situation
ğŸ‘‰ thought process
ğŸ‘‰ decision
ğŸ‘‰ why this is correct

â­ Scenario 1 â€” â€œWe need incremental pipeline. What should we use?â€

Problem

Data lands continuously in a staging table.
You only want new/changed rows â€” NOT full reloads.

Thinking

Full reload = expensive & slow

Manual comparison logic = error-prone

We need built-in change capture

Decision

ğŸ‘‰ Use Standard Stream + Task

Explanation

Stream tracks inserts, updates, deletes

Task runs on schedule and processes only deltas

Retry safe â€” if task fails, changes remain in stream

Result

âœ” cheaper
âœ” reliable
âœ” supports near-real-time ingestion

â­ Scenario 2 â€” â€œPipeline missed a day. How do we catch up?â€

Problem

Task stopped for 24 hours.
Stream now has too many changes OR stale risk.

Thinking

Streams rely on Time Travel.
If retention is still valid â€” changes still exist.

Decision

ğŸ‘‰ Use Time Travel to backfill missing period
ğŸ‘‰ Then resume stream processing

Why

Time Travel lets you recreate table at older timestamp and process missing data safely.

Lesson

Streams = â€œcurrent windowâ€
Time Travel = â€œhistorical safety netâ€

â­ Scenario 3 â€” â€œSomeone truncated table. Will stream recover data?â€

Problem

TRUNCATE removes all rows.

Thinking

Streams only track CDC events.
TRUNCATE bypasses change tracking.

Decision

ğŸ‘‰ Restore data using Time Travel or clone
ğŸ‘‰ Rebuild stream if needed

Key takeaway

Streams = track changes
They are NOT backups.

â­ Scenario 4 â€” â€œWe only append logs. Which stream?â€

Problem

Event logs never update or delete.

Thinking

Standard Stream is unnecessary overhead.

Decision

ğŸ‘‰ Use Append-Only Stream

Why

Cheaper metadata tracking

Smaller footprint

Simplest to manage

â­ Scenario 5 â€” â€œWe want very light CDC â€” only inserts ever.â€

Decision

ğŸ‘‰ Insert-Only Stream

Why

Snowflake doesnâ€™t spend time tracking deletes/updates you never use.

Rule

Choose the lightest stream type that still meets functional needs.

â­ Scenario 6 â€” â€œStream became STALE. What happened?â€

Problem

Error:

STREAM has become stale

Thinking

Stream needed Time Travel history that expired.

Decision

1ï¸âƒ£ recreate the stream
2ï¸âƒ£ if needed, clone table from older snapshot
3ï¸âƒ£ increase retention so it doesnâ€™t happen again

Lesson

Streams are NOT permanent.
They depend on Time Travel windows.

ğŸ”¹ 5ï¸âƒ£ MISCONCEPTIONS â€” FULL EXPLANATIONS
âŒ Misconception 1

â€œStreams store full copies of changed rows.â€

Reality

Streams store metadata + pointers, not data.

They reference Time Travel snapshots instead of duplicating storage.

âœ” cheaper
âœ” faster
âœ” always consistent

âŒ Misconception 2

â€œStreams keep history forever.â€

Reality

History exists only as long as Time Travel retention exists.

Once retention expires:

stream may become stale

missing history cannot be replayed

you must recreate or backfill

âŒ Misconception 3

â€œStreams can rewind backwards.â€

Reality

Streams only move forward.

Reading advances consumption marker permanently.

If you accidentally process twice:

ğŸ‘‰ recover using clone or backfill logic â€” not by rewinding stream.

âŒ Misconception 4

â€œStreams replace Time Travel.â€

Reality

Streams depend on Time Travel.

Think of roles:

Feature	Purpose
Streams	incremental data changes
Time Travel	historical snapshots & recovery

They complement each other â€” not substitutes.

âŒ Misconception 5

â€œStreams show who changed the data.â€

Reality

Streams show what changed, not who.

To track WHO, combine:

QUERY_HISTORY

ACCESS_HISTORY

Streams are technical CDC â€” not audit logs.

âŒ Misconception 6

â€œStreams are good for full reload jobs.â€

Reality

Streams are designed ONLY for incremental processing.

If you reload full table regularly:

ğŸ‘‰ disable stream during reload
ğŸ‘‰ or process from base table instead

Reload + stream together = confusion + data duplication.